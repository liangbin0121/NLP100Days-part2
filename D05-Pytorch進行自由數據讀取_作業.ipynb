{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"cupoy_env","language":"python","name":"cupoy_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"D05-Pytorch進行自由數據讀取_作業.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"d2SjU4Rl2p62"},"source":["### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n","\n","本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n","客製化資料讀取。\n","下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n","(請同學先行至IMDB下載資料)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnZ6wLhr3YSZ","executionInfo":{"status":"ok","timestamp":1617122267870,"user_tz":-480,"elapsed":631,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}},"outputId":"e4ee9abb-3c90-461b-e59a-631d03a62a14"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-uhv-9U3YU_","executionInfo":{"status":"ok","timestamp":1617122270742,"user_tz":-480,"elapsed":783,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}},"outputId":"c911bfba-e214-4177-9469-296f194d1768"},"source":["import os\n","\n","# Current directory\n","print(os.getcwd())\n","\n","# change directory\n","os.chdir('/content/drive/MyDrive/python_training/NLP100Days-part2/D05_Pytorch_dataset_free_read')\n","print(os.getcwd())"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n","/content/drive/MyDrive/python_training/NLP100Days-part2/D05_Pytorch_dataset_free_read\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"08yJL4tR2p67"},"source":["### 載入套件"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZoeT8Qp2p68","executionInfo":{"status":"ok","timestamp":1617122274247,"user_tz":-480,"elapsed":1728,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}},"outputId":"bbdb2ea4-41fc-4715-fe3b-02644f3df7cc"},"source":["# Import torch and other required modules\n","import nltk\n","nltk.download('stopwords') #下載stopwords\n","nltk.download('punkt') #下載word_tokenize需要的corpus\n","\n","import glob\n","import torch\n","import re\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.datasets import load_svmlight_file\n","from nltk.corpus import stopwords\n","from torch.utils.data import Dataset, DataLoader, RandomSampler\n","from torch.nn.utils.rnn import pad_sequence\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yJxs_xE72p69"},"source":["### 探索資料與資料前處理\n","這份作業我們使用test資料中的pos與neg\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5nRI8Iri2p69","executionInfo":{"status":"ok","timestamp":1617122276624,"user_tz":-480,"elapsed":745,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}},"outputId":"b3183b53-e506-4c06-d50f-8b98efc25ba9"},"source":["# 讀取字典，這份字典為review內所有出現的字詞\n","###<your code>###\n","with open(os.path.join('../dataset/aclImdb', 'imdb.vocab'), encoding='utf-8') as f:\n","    vocab = [line.strip() for line in f.readlines()]\n","\n","# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n","print(f\"vocab length before removing stopwords: {len(vocab)}\")\n","### <your code> ###\n","en_stopwords = set(stopwords.words('english'))\n","vocab = [word for word in vocab if word not in en_stopwords]\n","\n","print(f\"vocab length after removing stopwords: {len(vocab)}\")\n","\n","# 將字典轉換成dictionary\n","### <your code> ###\n","vocab_dic = {word: idx for idx, word in enumerate(vocab)}"],"execution_count":4,"outputs":[{"output_type":"stream","text":["vocab length before removing stopwords: 89527\n","vocab length after removing stopwords: 89356\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hVo_8A4X2p6-","executionInfo":{"status":"ok","timestamp":1617122377202,"user_tz":-480,"elapsed":94355,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}},"outputId":"419e5ead-64c3-46c3-ce4d-1a1e97b9e500"},"source":["# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n","# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n","# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n","\n","### <your code> ###\n","#/aclImdb/train/pos/11562_9.txt\n","review_pairs = []\n","\n","review_pos = glob.glob(\"../dataset/aclImdb/train/pos/*.txt\")\n","review_neg = glob.glob(\"../dataset/aclImdb/train/neg/*.txt\")\n","review_all = review_pos + review_neg\n","y = [1]*len(review_pos) + [0]*len(review_neg)\n","review_pairs = list(zip(review_all, y))\n","\n","print(review_pairs[:2])\n","print(f\"Total reviews: {len(review_pairs)}\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[('../dataset/aclImdb/train/pos/11562_9.txt', 1), ('../dataset/aclImdb/train/pos/11581_10.txt', 1)]\n","Total reviews: 25000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7DcERVlW2p6-"},"source":["### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n","這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n","(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"]},{"cell_type":"code","metadata":{"id":"bytZGwoH2p6_","executionInfo":{"status":"ok","timestamp":1617122393075,"user_tz":-480,"elapsed":628,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}}},"source":["def load_review(review_path):\n","    \n","    ###<your code>###\n","    with open(review_path, encoding='utf-8') as f:\n","        review = f.read()\n","    review = re.sub(r'\\W', ' ', review)\n","    review = nltk.word_tokenize(review)    \n","    return review\n","\n","def generate_vec(review, vocab_dic):\n","    ### <your code> ###\n","    bag_vector = np.zeros(len(vocab_dic))\n","    for word in review:\n","        if vocab_dic.get(word):\n","            bag_vector[vocab_dic.get(word)] += 1\n","            \n","    return bag_vector"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVNjGp_z2p6_","executionInfo":{"status":"ok","timestamp":1617122397920,"user_tz":-480,"elapsed":658,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}}},"source":["#建立客製化dataset\n","\n","class dataset(Dataset):\n","    '''custom dataset to load reviews and labels\n","    Parameters\n","    ----------\n","    data_pairs: list\n","        directory of all review-label pairs\n","    vocab: list\n","        list of vocabularies\n","    '''\n","    ### <your code> ###\n","    def __init__(self, data_dirs, vocab):\n","        ###<your code>###\n","        self.data_dirs = data_dirs\n","        self.vocab = vocab\n","\n","    def __len__(self):\n","        ###<your code>###\n","        return len(self.data_dirs)\n","\n","    def __getitem__(self, idx):\n","        ###<your code>###\n","        review_path, label = self.data_dirs[idx]\n","        review = load_review(review_path)\n","        bag_vector = generate_vec(review, self.vocab)\n","\n","        return bag_vector, label\n","    \n","\n","#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n","def collate_fn(batch):\n","    ### <your code> ###\n","    reviews, labels = zip(*batch)\n","    lengths = torch.LongTensor([len(review) for review in reviews])\n","    labels = torch.LongTensor(labels)\n","    reviews = pad_sequence([\n","        torch.LongTensor(review) for review in reviews\n","    ], batch_first=True, padding_value=0)\n","\n","    return reviews, labels, lengths"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"N_2aEz_S2p6_","executionInfo":{"status":"ok","timestamp":1617122401032,"user_tz":-480,"elapsed":1682,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}},"outputId":"79f4618c-a6e7-4d41-e5f6-b949a9c163fc"},"source":["# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n","### <your code> ###\n","custom_dst = dataset(review_pairs, vocab_dic)\n","custom_dataloader = DataLoader(custom_dst, \n","                 batch_size=4, \n","                 sampler=RandomSampler(custom_dst), \n","                 collate_fn=collate_fn)\n","\n","next(iter(custom_dataloader))"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0, 0, 0,  ..., 0, 0, 0],\n","         [0, 8, 1,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 1,  ..., 0, 0, 0]]),\n"," tensor([1, 1, 0, 0]),\n"," tensor([89356, 89356, 89356, 89356]))"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"uk7f9CHF2p7A"},"source":[""],"execution_count":null,"outputs":[]}]}